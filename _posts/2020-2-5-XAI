Research progress in XAI has been rapidly advancing, from input attribution (LIME, Anchors, LOCO, SHAP, DeepLift, Integrated Gradients, XRAI etc.), concept testing/extraction (TCAV, DeepR, Towards Automatic Concept-based Explanations), example influence/matching (MMD Critic, Representer Point Selection, Influence Functions, Attention-Based Prototypical Learning), distillation (Distilling the Knowledge in a Neural Network, Distilling a Neural Network Into a Soft Decision Tree). 
Furthermore, we continue to see novel approaches to inherently interpretable and controllable models like Deep Lattice Networks and Bayesian models.

딥러닝은 일반적으로 모델정확도와 모델설명도 사이의 트레이드오프 관계를 갖습니다. 일반적으로 레이어가 깊어질 수록 정확도는 높아집니다. 하지만, 선형/비선형 관계 또한 증가하게 됩니다. 비선형 관계의 증가로 인해 각 Feature(독립변수)가 prediction(종속변수)에 어느정도 영향을 미치는 지 해석하기가 어려워졌습니다. 이로 인해 딥러닝 모델들을 블랙박스라고 부르곤 합니다. 
수자원 분야에서도 딥러닝 모델 예측의 근거를 설명할 수 없다면, 의사결정자는 딥러닝 모델의 예측값에 근거해서 정책을 수립할 수 없을 것입니다. 사람들 역시 믿음의 근거를 제시해주지 않는다면 그 의사결정을 따르려 하지 않을 것입니다.

딥러닝 모델을 설명하는 방법은 여러 가지 종류가 있습니다. input feature(독립변수)들이 prediction(종속변수)에 미치는 영향도를 locally 추정하는 방법(LIME, Anchors, LOCO, SHAP, DeepLift, Integrated Gradients, XRAI etc.), 사전에 정의된 concept들을 이용해서 prediction을 설명하는 방법(TCAV, DeepR, Towards Automatic Concept-based Explanations), 학습에 사용된 예제들의 정보를 이용해서 설명하는 방법(MMD Critic, Representer Point Selection, Influence Functions, Attention-Based Prototypical Learning), 복잡한 딥러닝 모델을 이용하여 설명 가능한 모델의 형태(e.g. Decision Tree)를 globally 학습시키는 방법(Distilling the Knowledge in a Neural Network, Distilling a Neural Network Into a Soft Decision Tree), 해석 기능을 가지고 있는 복잡한 모형을 활용하는 방법(Deep Lattice Networks and Bayesian models) 등으로 구분할 수 있습니다. (출처: https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf)

다양한 방법들 중 수자원 분야에 적합하고, 가장 simple하게 적용할 수 있는 방법인 input feature(독립변수)들이 prediction(종속변수)에 미치는 영향도를 locally 추정하는 방법인 LIME과 SHAP에 대해 간단히 설명드리려 합니다. 이 두 방법은 모두 복잡한 모델이 어떤 알고리즘(딥러닝, SVM, Logistic Regression 등)을 써서 만들어졌더라도 적용할 수 있는 Model Agnostic Methods입니다.

먼저 LIME은 Local Interpretable Model-Agnostic Explanatons의 약어로, Locally 해석가능한 간단한 선형모델을 찾는 방법입니다. 즉, 복잡한 모델을 설명할 수 있는 단 하나의 간단한 모델(e.g. 선형 모델, Decision Tree)을 만드는 것이 아니라, 하나의 input data에 대해서 그 data의 결과가 어떤 feature들로 인해 결정됬는 지 설명할 수 있는 data 하나 만을 위한 모델을 찾는 방법입니다. 이를 위해 복잡한 원래 모델의 한 지점에서의 행동을 모사할 간단한 선형 모델을 정의합니다. 선형 모델의 독립변수들은 원래 독립변수의 값을 그대로 사용하는 것이 아니라 binary값(0 or 1)으로 바꾸어 사용합니다. input feature들의 원래 값을 복잡한 모델에 넣어 예측된 결과값과 가장 유사한 결과를 갖는 단순한 모델(대부분의  binary값이 0이고 정말 유의미한 feature들만 1)을 찾습니다. 이 때 단 하나의 예제만을 가지고 

즉 선형 모델에서 feature값이 1이 되면 그 feature는 그 데이터를 설명할 수 있는 feature이며, 그 feature의 계수값이 정도가 됩니다. 

{A, B,C} 

 ```
● v({}) = 0
● v({A}) = 10
● v({B}) = 20
● v({C}) = 30
● v({A, B}) = 60
● v({B,C}) = 70
● v({A,C}) = 90
● v({A, B,C}) = 100
```

```
1. {} → {A} → {A, B} → {A, B, C}
2. {} → {A} → {A, C} → {A, B, C}
3. {} → {B} → {A, B} → {A, B, C}
4. {} → {B} → {B, C} → {A, B, C}
5. {} → {C} → {B, C} → {A, B, C}
6. {} → {C} → {A, C} → {A, B, C}
```

```
● A: v({A}) − v({}) = 10 − 0 = 10
● B: v({A, B}) − v({A}) = 60 − 10 = 50
● C: v({A, B,C}) − v({A, B}) = 100 − 60 = 40
```

```
1. {} → {A} → {A, B} → {A, B, C} || A = 10, B = 50, C = 40
2. {} → {A} → {A, C} → {A, B, C} || A = 10, B = 10, C = 80
3. {} → {B} → {A, B} → {A, B, C} || A = 40, B = 20, C = 40
4. {} → {B} → {B, C} → {A, B, C} || A = 30, B = 20, C = 50
5. {} → {C} → {B, C} → {A, B, C} || A = 30, B = 40, C = 30
6. {} → {C} → {A, C} → {A, B, C} || A = 60, B = 10, C = 30
```

A_avg = 30 , B_avg = 25 , C_avg = 45

The Shapley value is the unique method that satisfies the following desirable axioms, which motivates its use and are the main reason we chose it to power our feature attributions offering:
1. Completeness (also known as efficiency): Shapley values sum up to the difference between the target outcome and the outcome of the foil. 30 + 25 + 45 = 100 - 0 (outcome of {})
2. Symmetry: For two participants i, j and S, where S is any subset of participants not including i, j, when {S, i} = {S, j} → i and j should have the same attribution value.
3. Dummy: For one participant i and S, where S is any subset of participants not including i, when {S, i} = {S} → i should get zero attribution.
4. Additivity: For two outcome functions, the Shapley values of the sum of the two outcome functions should be the sum of the Shapley values of the two outcome functions.






